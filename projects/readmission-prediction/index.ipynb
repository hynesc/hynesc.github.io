---
title: "Revisiting Neural Models for Hospital Readmission: A Replication and Extension Study"
author: 
  - name: "Christopher Hynes"
  - name: "Felipe Oliveira"
date: "April 2025" 
# image: thumbnail.png 
categories: [Python, Machine Learning, Deep Learning, Neural Networks]
---

[{{< fa brands github >}} GitHub](https://github.com/hynesc/readmission-prediction) | [{{< fa solid file-pdf >}} Read the Paper (PDF)](readmission-prediction.pdf)

---

*This page contains the executed Jupyter Notebook for the project, showing the code and its outputs. For a detailed explanation of the methodology and findings, please read the [full paper (PDF)](readmission-prediction.pdf). The complete source code and data are available in the [GitHub repository](https://github.com/hynesc/readmission-prediction).*

---

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUh-ROn5LKmf"
      },
      "source": [
        "## Config and Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mscCrWZKZqT"
      },
      "source": [
        "### Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMOxIapkKApX",
        "outputId": "907ebe17-7219-407d-bcc9-ae28ed02d780"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current Working *Directory*: /content\n",
            "Mounted at /content/gdrive\n",
            "Working Directory: /content/gdrive/My Drive/readmission-prediction/code\n"
          ]
        }
      ],
      "source": [
        "# View and modify the working path\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# View current working directory\n",
        "print(\"Current Working *Directory*:\", os.getcwd())\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Change working directory to your file position\n",
        "path = \"/content/gdrive/My Drive/readmission-prediction-main/code\"\n",
        "os.chdir(path)\n",
        "\n",
        "# Confirm the change\n",
        "print(\"Working Directory:\", os.getcwd())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux5UNMiL9AMU"
      },
      "source": [
        "### Initialize Paper (Xiao et al., 2018) Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TAs - for the purpose of faster runtime, we are intentionally setting the parameters \"num_epochs\" to 2, and \"num_trials\" to 2, even though Xiao et al. use 6 and 10, respectfully. To replicate the original paper, evaluation outputs you see at the end (See \"Executing Models\" Section) are from using the original 6 epoch / 10 trial parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KG_Q3ZnlLHdY",
        "outputId": "e16ed8a5-1f6c-488a-a921-734c7def09ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Config] Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from statistics import mean, stdev\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    precision_recall_fscore_support,\n",
        "    roc_auc_score,\n",
        "    accuracy_score,\n",
        "    precision_recall_curve,\n",
        ")\n",
        "from sklearn.metrics import average_precision_score as pr_auc\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import zipfile\n",
        "\n",
        "class Config:\n",
        "    \"\"\"\n",
        "    Holds hyperparameters, file paths, and general settings.\n",
        "    In practice, you could store these in a YAML/JSON file.\n",
        "    \"\"\"\n",
        "    # Data paths\n",
        "    dataset_dir = \"../resource\"\n",
        "    zipped_file = os.path.join(dataset_dir, \"S1_Data.zip\")\n",
        "    input_file  = os.path.join(dataset_dir, \"S1_Data.txt\")  # after unzipping\n",
        "\n",
        "\n",
        "    vocab_file = os.path.join(dataset_dir, \"vocab.txt\")\n",
        "    stop_file = os.path.join(dataset_dir, \"stop.txt\")\n",
        "    vocab_pkl = os.path.join(dataset_dir, \"vocab.pkl\")\n",
        "\n",
        "    # PKLs for train, valid, test data\n",
        "    pkl_train_x = os.path.join(dataset_dir, \"X_train.pkl\")\n",
        "    pkl_train_y = os.path.join(dataset_dir, \"Y_train.pkl\")\n",
        "    pkl_val_x   = os.path.join(dataset_dir, \"X_valid.pkl\")\n",
        "    pkl_val_y   = os.path.join(dataset_dir, \"Y_valid.pkl\")\n",
        "    pkl_test_x  = os.path.join(dataset_dir, \"X_test.pkl\")\n",
        "    pkl_test_y  = os.path.join(dataset_dir, \"Y_test.pkl\")\n",
        "\n",
        "    # For building the vocab\n",
        "    rare_word_threshold = 100\n",
        "    stop_word_threshold = 1e4\n",
        "\n",
        "    unknown_index = 1\n",
        "    vocab_size = 490\n",
        "    n_stops = 12  # last 12 are considered \"stop words\"\n",
        "    n_topics = 50\n",
        "    max_visit_len = 300\n",
        "\n",
        "    # Model hyperparams\n",
        "    embed_size = 100\n",
        "    hidden_size = 200\n",
        "\n",
        "    # Training settings\n",
        "    batch_size = 1\n",
        "    grad_clip = 100\n",
        "    learning_rate = 0.001\n",
        "    num_epochs = 2\n",
        "    num_trials = 2\n",
        "\n",
        "    # Where to save intermediate outputs\n",
        "    content_theta_dir = \"../output/CONTENT_theta\"\n",
        "    gru_hiddens_dir = \"../output/GRU_hiddens\"\n",
        "    content_results_dir = \"../output/CONTENT_results\"\n",
        "    gru_results_dir = \"../output/GRU_results\"\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"[Config] Device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU0aE6IqeB7n"
      },
      "source": [
        "### Execute Config setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnaJBPF5eAQE"
      },
      "outputs": [],
      "source": [
        "config = Config()\n",
        "\n",
        "os.makedirs(config.content_theta_dir, exist_ok=True)\n",
        "os.makedirs(config.gru_hiddens_dir, exist_ok=True)\n",
        "os.makedirs(config.content_results_dir, exist_ok=True)\n",
        "os.makedirs(config.gru_results_dir, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYSvC3LvtsNE"
      },
      "source": [
        "## Data Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fih4kxz0LTNB"
      },
      "source": [
        "###  Utility Helpers (Pickle, Numpy, Vocab, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mFuj04GzLVsY"
      },
      "outputs": [],
      "source": [
        "def save_pkl(path, obj):\n",
        "    with open(path, \"wb\") as f:\n",
        "        pickle.dump(obj, f)\n",
        "    print(f\"Saved: {path}\")\n",
        "\n",
        "def load_pkl(path):\n",
        "    with open(path, \"rb\") as f:\n",
        "        obj = pickle.load(f)\n",
        "    print(f\"Loaded: {path}\")\n",
        "    return obj\n",
        "\n",
        "def save_npy(path, arr):\n",
        "    np.save(path, arr)\n",
        "    print(f\"Saved: {path}\")\n",
        "\n",
        "def load_npy(path):\n",
        "    arr = np.load(path, allow_pickle=True)\n",
        "    print(f\"Loaded: {path}\")\n",
        "    return arr\n",
        "\n",
        "def ensure_data_unzipped(config: Config):\n",
        "    \"\"\"\n",
        "    Checks if the unzipped S1_Data.txt exists. If not, unzips S1_Data.zip.\n",
        "    Sets config.input_file to the unzipped file.\n",
        "    \"\"\"\n",
        "    if os.path.exists(config.input_file):\n",
        "        print(f\"S1_Data.txt already unzipped at: {config.input_file}\")\n",
        "        return\n",
        "    with zipfile.ZipFile(config.zipped_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(config.dataset_dir)\n",
        "    print(f\"Unzipped {config.zipped_file} => {config.dataset_dir}\")\n",
        "\n",
        "def build_vocab(config: Config):\n",
        "    \"\"\"\n",
        "    Creates vocab.txt and stop.txt from S1_Data.txt by filtering.\n",
        "    Index offset so that 'unknown_index' can be used.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(config.input_file, sep=\"\\t\", header=0)\n",
        "    grouped = df.groupby(\"DX_GROUP_DESCRIPTION\").size().reset_index(name=\"SIZE\")\n",
        "\n",
        "    # Filter out rare\n",
        "    grouped = grouped[grouped[\"SIZE\"] > config.rare_word_threshold]\n",
        "\n",
        "    # Sort by frequency ascending\n",
        "    grouped = grouped.sort_values(by=\"SIZE\").reset_index(drop=True)\n",
        "    vocab = grouped[\"DX_GROUP_DESCRIPTION\"]\n",
        "    vocab.index += 2  # offset => index=1 is reserved for unknown\n",
        "\n",
        "    vocab.to_csv(config.vocab_file, sep=\"\\t\", header=False, index=True)\n",
        "    print(\"Number of valid tokens:\", len(vocab))\n",
        "\n",
        "    # Stop words => extremely frequent\n",
        "    stops = grouped[grouped[\"SIZE\"] > config.stop_word_threshold]\n",
        "    stops[\"DX_GROUP_DESCRIPTION\"].to_csv(config.stop_file, sep=\"\\t\", header=False, index=False)\n",
        "\n",
        "def load_vocab_dict(config: Config):\n",
        "    \"\"\"\n",
        "    Reads vocab_file => returns {word: index}, also pickles reverse mapping.\n",
        "    \"\"\"\n",
        "    word_to_index = {}\n",
        "    with open(config.vocab_file, \"r\") as f:\n",
        "        for line in f:\n",
        "            idx_str, token = line.strip().split(\"\\t\")\n",
        "            word_to_index[token] = int(idx_str) - 1\n",
        "\n",
        "    reverse_mapping = {v: k for k, v in word_to_index.items()}\n",
        "    save_pkl(config.vocab_pkl, reverse_mapping)\n",
        "    print(f\"Vocab size: {len(word_to_index)}\")\n",
        "    return word_to_index\n",
        "\n",
        "def loadEmbeddingMatrix(wordvecFile, word_to_index, vocab_size):\n",
        "  # Build reverse mapping: index -> word\n",
        "  with open(wordvecFile, \"r\") as fw:\n",
        "      # Read the header: total number of words and embedding dimension\n",
        "      header = fw.readline().strip().split()\n",
        "      total, dim = int(header[0]), int(header[1])\n",
        "      # Initialize the embedding matrix with zeros\n",
        "      W = np.zeros((vocab_size, dim), dtype=np.float32)\n",
        "      for line in fw:\n",
        "          parts = line.strip().split()\n",
        "          # Reconstruct the word (in case it contains spaces)\n",
        "          word = \" \".join(parts[:-dim])\n",
        "          vec = np.array(parts[-dim:], dtype=np.float32)\n",
        "          try:\n",
        "              token_value = word_to_index[word]  # Get the token index from the reverse mapping\n",
        "          except KeyError:\n",
        "              print(f\"{word} is not in vocabulary; skipping.\")\n",
        "              continue\n",
        "          # Adjust index if your vocab mapping is 1-indexed; here we subtract 1.\n",
        "          W[token_value - 1] = vec\n",
        "  return W\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3aL1dloLX-F"
      },
      "source": [
        "### Data Preprocessing and Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yrBqRF-MLbO3"
      },
      "outputs": [],
      "source": [
        "def extract_inpatient_events(config: Config):\n",
        "    \"\"\"\n",
        "    Extracts 'INPATIENT HOSPITAL' events => used to mark readmission (1) if next event is within 30 days.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(config.input_file, sep=\"\\t\", header=0)\n",
        "    inpat = df[df[\"SERVICE_LOCATION\"] == \"INPATIENT HOSPITAL\"]\n",
        "    grouped = (inpat.groupby([\"PID\", \"DAY_ID\", \"SERVICE_LOCATION\"])\n",
        "                    .size()\n",
        "                    .reset_index(name=\"COUNT\")\n",
        "                    .sort_values([\"PID\", \"DAY_ID\"], ascending=True)\n",
        "                    .set_index(\"PID\"))\n",
        "    return grouped\n",
        "\n",
        "def convert_format(config: Config, word2idx, inpat_df):\n",
        "    \"\"\"\n",
        "    Goes through S1_Data.txt line by line => builds docs & labels.\n",
        "    docs[i] = [ [codes_on_day1], [codes_on_day2], ... ]\n",
        "    labels[i] = [0/1 for each day]\n",
        "    \"\"\"\n",
        "    def is_readmitted(pid, day):\n",
        "        try:\n",
        "            recs = inpat_df.loc[int(pid)]\n",
        "            if isinstance(recs, pd.Series):\n",
        "                return (int(day) <= recs.DAY_ID < int(day) + 30)\n",
        "            # Else a sub-DataFrame\n",
        "            subset = recs.loc[(int(day) <= recs.DAY_ID) & (recs.DAY_ID < int(day) + 30)]\n",
        "            return subset.shape[0] > 0\n",
        "        except KeyError:\n",
        "            return False\n",
        "\n",
        "    docs, labels = [], []\n",
        "    with open(config.input_file, \"r\") as f:\n",
        "        header = f.readline().strip().split(\"\\t\")\n",
        "        col_idx = {h: i for i, h in enumerate(header)}\n",
        "\n",
        "        doc, visit_codes, label_seq = [], [], []\n",
        "        line = f.readline()\n",
        "        if not line:\n",
        "            return docs, labels\n",
        "\n",
        "        tokens = line.strip().split(\"\\t\")\n",
        "        pid, day_id = tokens[col_idx[\"PID\"]], tokens[col_idx[\"DAY_ID\"]]\n",
        "        label_seq.append(1 if is_readmitted(pid, day_id) else 0)\n",
        "\n",
        "        while line:\n",
        "            tokens = line.strip().split(\"\\t\")\n",
        "            c_pid, c_day = tokens[col_idx[\"PID\"]], tokens[col_idx[\"DAY_ID\"]]\n",
        "\n",
        "            if c_pid != pid:\n",
        "                doc.append(visit_codes)\n",
        "                docs.append(doc)\n",
        "                labels.append(label_seq)\n",
        "                # Reset\n",
        "                doc, visit_codes, label_seq = [], [], []\n",
        "                pid, day_id = c_pid, c_day\n",
        "                label_seq.append(1 if is_readmitted(pid, day_id) else 0)\n",
        "            else:\n",
        "                # Same patient, check if new day\n",
        "                if c_day != day_id:\n",
        "                    doc.append(visit_codes)\n",
        "                    visit_codes = []\n",
        "                    day_id = c_day\n",
        "                    label_seq.append(1 if is_readmitted(pid, day_id) else 0)\n",
        "\n",
        "            diag_str = tokens[col_idx[\"DX_GROUP_DESCRIPTION\"]]\n",
        "            diag_idx = word2idx.get(diag_str, config.unknown_index)\n",
        "            visit_codes.append(diag_idx)\n",
        "            line = f.readline()\n",
        "\n",
        "        # Finalize\n",
        "        doc.append(visit_codes)\n",
        "        docs.append(doc)\n",
        "        labels.append(label_seq)\n",
        "\n",
        "    return docs, labels\n",
        "\n",
        "def split_and_save(config: Config, docs, labels):\n",
        "    \"\"\"\n",
        "    Splits docs/labels => train/valid/test and saves as pkl.\n",
        "    \"\"\"\n",
        "    # Adjust these splits as needed. We match the authors\n",
        "    train_end = 2000\n",
        "    val_end = 2500\n",
        "\n",
        "    save_pkl(config.pkl_train_x, docs[:train_end])\n",
        "    save_pkl(config.pkl_train_y, labels[:train_end])\n",
        "\n",
        "    save_pkl(config.pkl_val_x, docs[train_end:val_end])\n",
        "    save_pkl(config.pkl_val_y, labels[train_end:val_end])\n",
        "\n",
        "    save_pkl(config.pkl_test_x, docs[val_end:])\n",
        "    save_pkl(config.pkl_test_y, labels[val_end:])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pKvHSFILcv8"
      },
      "source": [
        "### PyTorch Dataset & DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3a7gCxvtLgJt"
      },
      "outputs": [],
      "source": [
        "class PatientVisitsDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset that holds (docs, labels) for a split (train/valid/test).\n",
        "    We'll convert them to multi-hot within __getitem__ or in a collate_fn.\n",
        "    \"\"\"\n",
        "    def __init__(self, docs, labels, vocab_size, max_len):\n",
        "        super().__init__()\n",
        "        self.docs = docs\n",
        "        self.labels = labels\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.docs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.docs[idx], self.labels[idx]\n",
        "\n",
        "def multi_hot_collate_fn(batch, vocab_size, max_len):\n",
        "    \"\"\"\n",
        "    Collate function to transform a list of (doc, label) => (x, y, mask).\n",
        "    Each doc is a list of visits.\n",
        "    We'll multi-hot each visit, padding each sample to a fixed length (max_len).\n",
        "    \"\"\"\n",
        "    batch_size = len(batch)\n",
        "\n",
        "    # 1) Separate docs and labels.\n",
        "    docs = [b[0] for b in batch]\n",
        "    labels = [b[1] for b in batch]\n",
        "\n",
        "    # 2) Enforce each sample to have exactly max_len visits (truncating if longer, padding with zeros if shorter).\n",
        "    x_array = np.zeros((batch_size, max_len, vocab_size), dtype=np.float32)\n",
        "    y_array = np.ones((batch_size, max_len), dtype=np.float32)\n",
        "    mask_array = np.zeros((batch_size, max_len), dtype=np.float32)\n",
        "\n",
        "    for i, (doc, lab) in enumerate(zip(docs, labels)):\n",
        "        # Use the minimum between the number of visits in the doc and max_len.\n",
        "        seq_len = min(len(doc), max_len)\n",
        "        mask_array[i, :seq_len] = 1\n",
        "        y_array[i, :seq_len] = lab[:seq_len]\n",
        "        for j in range(seq_len):\n",
        "            visit_codes = doc[j]\n",
        "            for code_idx in visit_codes:\n",
        "                # Adjust for 0-indexed coding; your vocabulary indices seem to be 1-indexed.\n",
        "                x_array[i, j, code_idx - 1] = 1\n",
        "\n",
        "    # Convert numpy arrays to torch tensors.\n",
        "    x_tensor = torch.from_numpy(x_array)\n",
        "    y_tensor = torch.from_numpy(y_array)\n",
        "    mask_tensor = torch.from_numpy(mask_array)\n",
        "    return x_tensor, y_tensor, mask_tensor\n",
        "\n",
        "\n",
        "def create_dataloader(docs, labels, config: Config, shuffle=False):\n",
        "    \"\"\"\n",
        "    Convenience method to build a DataLoader from docs/labels.\n",
        "    \"\"\"\n",
        "    dataset = PatientVisitsDataset(docs, labels, config.vocab_size, config.max_visit_len)\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=shuffle,\n",
        "        collate_fn=lambda b: multi_hot_collate_fn(b, config.vocab_size, config.max_visit_len),\n",
        "    )\n",
        "    return loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2Py4FclNbtw"
      },
      "source": [
        "### Executing Data Prep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv2VOHih7phB"
      },
      "source": [
        "You can comment out 1 and 2 if already run - loads existing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJFXK01cNhXr",
        "outputId": "8ffaa142-fcea-4330-fbd4-59a72c72665c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "S1_Data.txt already unzipped at: ../resource/S1_Data.txt\n",
            "Number of valid tokens: 490\n",
            "Saved: ../resource/vocab.pkl\n",
            "Vocab size: 490\n",
            "Saved: ../resource/X_train.pkl\n",
            "Saved: ../resource/Y_train.pkl\n",
            "Saved: ../resource/X_valid.pkl\n",
            "Saved: ../resource/Y_valid.pkl\n",
            "Saved: ../resource/X_test.pkl\n",
            "Saved: ../resource/Y_test.pkl\n",
            "Loaded: ../resource/X_train.pkl\n",
            "Loaded: ../resource/Y_train.pkl\n",
            "Loaded: ../resource/X_valid.pkl\n",
            "Loaded: ../resource/Y_valid.pkl\n",
            "Loaded: ../resource/X_test.pkl\n",
            "Loaded: ../resource/Y_test.pkl\n"
          ]
        }
      ],
      "source": [
        "# 1) Build vocab & Load it\n",
        "ensure_data_unzipped(config)\n",
        "build_vocab(config)\n",
        "w2i = load_vocab_dict(config)\n",
        "\n",
        "# 2) Extract events => docs => split => pkl\n",
        "inpat_events = extract_inpatient_events(config)\n",
        "docs, labels = convert_format(config, w2i, inpat_events)\n",
        "split_and_save(config, docs, labels)\n",
        "\n",
        "# 3) Load those splits into memory\n",
        "X_train = load_pkl(config.pkl_train_x)\n",
        "Y_train = load_pkl(config.pkl_train_y)\n",
        "\n",
        "X_valid = load_pkl(config.pkl_val_x)\n",
        "Y_valid = load_pkl(config.pkl_val_y)\n",
        "\n",
        "X_test  = load_pkl(config.pkl_test_x)\n",
        "Y_test  = load_pkl(config.pkl_test_y)\n",
        "\n",
        "# 4) Create DataLoaders\n",
        "train_loader = create_dataloader(X_train, Y_train, config, shuffle=True)\n",
        "valid_loader = create_dataloader(X_valid, Y_valid, config, shuffle=False)\n",
        "test_loader  = create_dataloader(X_test,  Y_test,  config, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZBu34j2slPh"
      },
      "source": [
        "### Executing Label Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6Wpc5zcrnuJ",
        "outputId": "1d08867c-13af-4495-ac50-76b64123280d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label 1 has 54427 counts, and Label 0 has 185509 counts\n"
          ]
        }
      ],
      "source": [
        "label_distribution = {1: 0, 0: 0}\n",
        "for l in labels:\n",
        "  for num in l:\n",
        "    label_distribution[num] += 1\n",
        "print(\"Label 1 has {} counts, and Label 0 has {} counts\".format(label_distribution[1], label_distribution[0]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6umc8Ym6Li_d"
      },
      "source": [
        "### Defining CONTENT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUAIzhoILlv7"
      },
      "outputs": [],
      "source": [
        "class ThetaLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Reparameterization for the topic distribution + KL term.\n",
        "    \"\"\"\n",
        "    def __init__(self, max_len, n_topics):\n",
        "        super().__init__()\n",
        "        self.max_len = max_len\n",
        "        self.kl_term = 0.0\n",
        "        self.theta = None\n",
        "        self.n_topics = n_topics\n",
        "\n",
        "    def forward(self, mu, log_sigma):\n",
        "        eps = torch.randn_like(mu)\n",
        "        z = mu + torch.exp(0.5 * log_sigma) * eps\n",
        "        theta = F.softmax(z, dim=1)\n",
        "        self.theta = theta\n",
        "\n",
        "        # kl\n",
        "        self.kl_term = -0.5 * torch.sum(1 + log_sigma - mu.pow(2) - log_sigma.exp())\n",
        "\n",
        "        self.batch_size_flat = mu.size(0)\n",
        "\n",
        "        # Expand => [batch_size, seq_len, n_topics]\n",
        "        expanded_theta = theta.unsqueeze(1).expand(-1, self.max_len, self.n_topics)\n",
        "        return expanded_theta\n",
        "\n",
        "        return expanded_theta\n",
        "\n",
        "\n",
        "class ContentModel(nn.Module):\n",
        "    \"\"\"\n",
        "    RNN + Topic model:\n",
        "      1) embed => GRU\n",
        "      2) dense => mu, log_sigma => Theta\n",
        "      3) B * Theta => context\n",
        "      4) sum => final prediction\n",
        "    \"\"\"\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.embed_size = config.embed_size\n",
        "        self.hidden_size= config.hidden_size\n",
        "        self.n_topics   = config.n_topics\n",
        "        self.max_len    = config.max_visit_len\n",
        "\n",
        "        self.embed = nn.Linear(self.vocab_size, self.embed_size, bias=False)\n",
        "        self.gru = nn.GRU(self.embed_size, self.hidden_size, batch_first=True)\n",
        "\n",
        "        self.dense1 = nn.Linear(self.vocab_size, self.hidden_size)\n",
        "        self.dense2 = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.mu = nn.Linear(self.hidden_size, self.n_topics)\n",
        "        self.log_sigma = nn.Linear(self.hidden_size, self.n_topics)\n",
        "\n",
        "        self.B = nn.Linear(self.vocab_size, self.n_topics, bias=False)\n",
        "        self.out_layer = nn.Linear(self.hidden_size, 1)\n",
        "\n",
        "        self.theta_layer = ThetaLayer(self.max_len, self.n_topics)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        B, T, _ = x.shape\n",
        "\n",
        "        # x => [batch, seq_len, vocab_size]\n",
        "        embedded = self.embed(x) * mask.unsqueeze(-1)  # [B, T, embed_size]\n",
        "        gru_out, h_n = self.gru(embedded)\n",
        "\n",
        "        # Topic\n",
        "        h1 = F.relu(self.dense1(x))  # [B, T, hidden]\n",
        "        h2 = F.relu(self.dense2(h1)) # [B, T, hidden]\n",
        "\n",
        "        # Cumulative mean up to each time‑step\n",
        "        seq_lens = mask.sum(dim=1, keepdim=True)             # [B,1]\n",
        "        h2_mean  = (h2.sum(dim=1) / (seq_lens + 1e-8))       # [B,H]\n",
        "\n",
        "        mu        = self.mu(h2_mean)                         # [B,K]\n",
        "        log_sigma = self.log_sigma(h2_mean)                     # [B,K]\n",
        "\n",
        "        theta = self.theta_layer(mu, log_sigma)              # [B,T,K]\n",
        "\n",
        "        # Context\n",
        "        context = (self.B(x) * theta).mean(dim=-1)           # mean over K  ➜ [B,T]\n",
        "        rnn_scores = self.out_layer(gru_out).squeeze(-1)       # [B,T]\n",
        "\n",
        "        logits = rnn_scores + context                        # additive fusion\n",
        "        probs  = torch.sigmoid(logits)                       # [B,T]\n",
        "\n",
        "        # apply mask after sigmoid & keep tiny ε everywhere\n",
        "        probs = probs * mask + 1e-6\n",
        "\n",
        "        return probs, h_n, theta\n",
        "\n",
        "    @property\n",
        "    def kl_term(self):\n",
        "      return self.theta_layer.kl_term / (self.theta_layer.max_len + 1e-8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guCDJBEVvcWM"
      },
      "source": [
        "### Defining GRU Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBn5bs2vvlTz"
      },
      "outputs": [],
      "source": [
        "class GRUModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A basic GRU model without word embeddings or topic modeling:\n",
        "      1) direct input => GRU\n",
        "      2) GRU output => prediction\n",
        "    \"\"\"\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.hidden_size = config.hidden_size\n",
        "\n",
        "        # Direct GRU without embedding layer\n",
        "        self.gru = nn.GRU(self.vocab_size, self.hidden_size, batch_first=True)\n",
        "\n",
        "        # Output layer\n",
        "        self.out_layer = nn.Linear(self.hidden_size, 1)\n",
        "\n",
        "        # For compatibility with ContentModel API\n",
        "        self.kl_term = 0.0\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # x => [batch, seq_len, vocab_size]\n",
        "        # Apply mask to input\n",
        "        masked_input = x * mask.unsqueeze(-1)  # [B, T, vocab_size]\n",
        "\n",
        "        # Pass through GRU\n",
        "        gru_out, h_n = self.gru(masked_input)\n",
        "\n",
        "        # Generate scores\n",
        "        logits = self.out_layer(gru_out).squeeze(-1)  # [B, T]\n",
        "\n",
        "        # Apply sigmoid and mask\n",
        "        out = torch.sigmoid(logits)\n",
        "        out = out * mask  # zeros for padded positions\n",
        "        out = torch.clamp(out, min=1e-6, max=1 - 1e-6)  # ensure strictly in (0,1)\n",
        "\n",
        "        # For compatibility with ContentModel API, return None as theta\n",
        "        # Ensures compatibility with train() and evaluate_model()\n",
        "        return out, h_n, None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S93_mShgLnX3"
      },
      "source": [
        "### Defining Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAA6PVXgLvU4"
      },
      "outputs": [],
      "source": [
        "def train(model, loader, optimizer, config: Config):\n",
        "    \"\"\"\n",
        "    Train step over 'loader' => returns average train loss, plus any collected [theta+hidden].\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    batch_count= 0\n",
        "    collector  = []\n",
        "\n",
        "    for x_batch, y_batch, m_batch in loader:\n",
        "        x_batch = x_batch.to(config.device)\n",
        "        y_batch = y_batch.to(config.device)\n",
        "        m_batch = m_batch.to(config.device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Handle different model types\n",
        "        if isinstance(model, ContentModel):\n",
        "            preds, h_n, theta = model(x_batch, m_batch)\n",
        "            bce = F.binary_cross_entropy(preds, y_batch, reduction='none')\n",
        "            bce = (bce * m_batch).sum() / m_batch.sum()    # mean over real visits\n",
        "            loss = bce + model.kl_term               # add KL term from model\n",
        "\n",
        "            # Store [theta + hidden]\n",
        "            rnn_vec  = h_n.squeeze(0).detach().cpu().numpy()     # [B, H]\n",
        "            theta_np = theta[:, 0, :].detach().cpu().numpy() # [B, K]  ← collapse T\n",
        "            combined = np.concatenate([theta_np, rnn_vec], axis=1)  # [B, K+H]\n",
        "            collector.append(combined)\n",
        "\n",
        "        elif isinstance(model, GRUModel):\n",
        "            preds, h_n, _ = model(x_batch, m_batch)\n",
        "            bce = F.binary_cross_entropy(preds, y_batch, reduction='none')\n",
        "            bce = (bce * m_batch).sum() / m_batch.sum()    # mean over real visits\n",
        "            loss = bce\n",
        "\n",
        "            # Store just the hidden for GRU\n",
        "            rnn_vec = h_n.squeeze(0).detach().cpu().numpy()\n",
        "            collector.append(rnn_vec)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        batch_count+= 1\n",
        "\n",
        "    avg_loss = total_loss / max(batch_count,1)\n",
        "    return avg_loss, collector\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1kvtURJia5k"
      },
      "source": [
        "### Defining Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pPWSJdwiZ-P"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, loader, config: Config):\n",
        "    \"\"\"\n",
        "    Evaluates on a DataLoader (e.g. test/valid).\n",
        "    Returns (avg_loss, list_of_true, list_of_pred, theta_hidden_collector).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    batch_count= 0\n",
        "    all_true, all_pred = [], []\n",
        "    all_theta_hidden = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch, mask_batch in loader:\n",
        "            x_batch = x_batch.to(config.device)\n",
        "            y_batch = y_batch.to(config.device)\n",
        "            mask_batch = mask_batch.to(config.device)\n",
        "\n",
        "            if isinstance(model, ContentModel):\n",
        "                preds, h_n, theta = model(x_batch, mask_batch)\n",
        "                bce = F.binary_cross_entropy(preds, y_batch, reduction='none')\n",
        "                bce = (bce * mask_batch).sum() / mask_batch.sum()    # mean over real visits\n",
        "                loss = bce + model.kl_term               # add KL term from model\n",
        "\n",
        "                # Store [theta + hidden]\n",
        "                rnn_vec  = h_n.squeeze(0).detach().cpu().numpy()     # [B, H]\n",
        "                theta_np = theta[:, 0, :].detach().cpu().numpy() # [B, K]  ← collapse T\n",
        "                combined = np.concatenate([theta_np, rnn_vec], axis=1)  # [B, K+H]\n",
        "                all_theta_hidden.append(combined)\n",
        "\n",
        "            elif isinstance(model, GRUModel):\n",
        "                preds, h_n, _ = model(x_batch, mask_batch)\n",
        "                bce = F.binary_cross_entropy(preds, y_batch, reduction='none')\n",
        "                bce = (bce * mask_batch).sum() / mask_batch.sum()    # mean over real visits\n",
        "                loss = bce\n",
        "\n",
        "                # Store just the hidden for GRU\n",
        "                rnn_vec = h_n.squeeze(0).detach().cpu().numpy()\n",
        "                all_theta_hidden.append(rnn_vec)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            batch_count += 1\n",
        "\n",
        "            # flatten predictions/labels ignoring masked positions\n",
        "            seq_lens = mask_batch.sum(dim=1).cpu().numpy().astype(int)\n",
        "            preds_np = preds.detach().cpu().numpy()\n",
        "            y_np     = y_batch.detach().cpu().numpy()\n",
        "\n",
        "            for i in range(x_batch.shape[0]):\n",
        "                length_i = seq_lens[i]\n",
        "                all_pred.extend(preds_np[i, :length_i])\n",
        "                all_true.extend(y_np[i, :length_i])\n",
        "\n",
        "    avg_loss = total_loss / (batch_count if batch_count else 1)\n",
        "    return avg_loss, all_true, all_pred, all_theta_hidden\n",
        "\n",
        "\n",
        "def compute_metrics(true_vals, pred_vals):\n",
        "    \"\"\"\n",
        "    Returns a dict with AUC, PR-AUC, ACC, Precision, Recall, F1.\n",
        "    \"\"\"\n",
        "    auc_val = roc_auc_score(true_vals, pred_vals)\n",
        "    pr_val  = pr_auc(true_vals, pred_vals)\n",
        "    preds_bin = (np.array(pred_vals) > 0.5).astype(int)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(true_vals, preds_bin, average=\"binary\")\n",
        "    acc_val = accuracy_score(true_vals, preds_bin)\n",
        "    return {\n",
        "        \"auc\": auc_val,\n",
        "        \"prauc\": pr_val,\n",
        "        \"acc\": acc_val,\n",
        "        \"precision\": prec,\n",
        "        \"recall\": rec,\n",
        "        \"f1\": f1\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMybtDIMuUZk"
      },
      "source": [
        "## OPTIONAL Grid Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgSlY03-eX4i"
      },
      "source": [
        "### Defining Grid Search Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfBVtwQThCVx"
      },
      "outputs": [],
      "source": [
        "def grid_search(param_grid, model_type):\n",
        "  best_val_metric = 0\n",
        "  best_config = None\n",
        "\n",
        "  for hidden_size, batch_size, learning_rate, num_epochs, n_topics in itertools.product(\n",
        "          param_grid[\"hidden_size\"],\n",
        "          param_grid[\"batch_size\"],\n",
        "          param_grid[\"learning_rate\"],\n",
        "          param_grid[\"num_epochs\"],\n",
        "          param_grid[\"n_topics\"]\n",
        "      ):\n",
        "      print(\"Current parameters: \")\n",
        "      print(f\"Hidden Size: {hidden_size}\")\n",
        "      print(f\"Batch Size: {batch_size}\")\n",
        "      print(f\"Learning Rate: {learning_rate}\")\n",
        "      print(f\"Num Epochs: {num_epochs}\")\n",
        "      print(f\"Num Topics; {n_topics}\")\n",
        "\n",
        "      config = Config()  # Initialize a new configuration instance\n",
        "      config.hidden_size = hidden_size\n",
        "      config.batch_size = batch_size\n",
        "      config.learning_rate = learning_rate\n",
        "      config.num_epochs = num_epochs\n",
        "      config.n_topics = n_topics\n",
        "\n",
        "      # (Re)create your DataLoaders with the updated batch size if necessary\n",
        "      train_loader = create_dataloader(X_train, Y_train, config, shuffle=True)\n",
        "      valid_loader = create_dataloader(X_valid, Y_valid, config, shuffle=False)\n",
        "\n",
        "      # Build model and optimizer with these settings\n",
        "      model = model_type(config).to(config.device)\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "      train(model, train_loader, optimizer, config)  # Your training function call\n",
        "\n",
        "      best_val_metric_this_run = 0\n",
        "      val_loss, val_true, val_pred, val_theta_collector = evaluate_model(model, valid_loader, config)\n",
        "      val_metrics = compute_metrics(val_true, val_pred)\n",
        "      print(\"\\nPR-AUC: {:.4f}\".format(val_metrics[\"prauc\"]))\n",
        "      print(\"------------------\\n\")\n",
        "\n",
        "      if val_metrics[\"prauc\"] > best_val_metric_this_run: #using PR-AUC as metric\n",
        "          best_val_metric_this_run = val_metrics[\"prauc\"]\n",
        "\n",
        "      # Update the best overall configuration if the current run outperforms previous runs\n",
        "      if best_val_metric_this_run > best_val_metric:\n",
        "          best_val_metric = best_val_metric_this_run\n",
        "          best_config = {\n",
        "              \"hidden_size\": hidden_size,\n",
        "              \"batch_size\": batch_size,\n",
        "              \"learning_rate\": learning_rate,\n",
        "              \"num_epochs\": num_epochs,\n",
        "              \"n_topics\": n_topics\n",
        "          }\n",
        "\n",
        "  return best_val_metric, best_config\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eC-znEQIjXX1"
      },
      "source": [
        "### Executing Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kldmUG47g7pM",
        "outputId": "a0d19e5a-816b-4d5d-98f7-dc1dbfe5e75c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current parameters: \n",
            "Hidden Size: 100\n",
            "Batch Size: 1\n",
            "Learning Rate: 0.001\n",
            "Num Epochs: 2\n",
            "Num Topics; 50\n",
            "\n",
            "PR-AUC: 0.6432\n",
            "------------------\n",
            "\n",
            "Current parameters: \n",
            "Hidden Size: 100\n",
            "Batch Size: 1\n",
            "Learning Rate: 0.001\n",
            "Num Epochs: 2\n",
            "Num Topics; 150\n",
            "\n",
            "PR-AUC: 0.6440\n",
            "------------------\n",
            "\n",
            "Current parameters: \n",
            "Hidden Size: 100\n",
            "Batch Size: 1\n",
            "Learning Rate: 0.001\n",
            "Num Epochs: 6\n",
            "Num Topics; 50\n",
            "\n",
            "PR-AUC: 0.6457\n",
            "------------------\n",
            "\n",
            "Current parameters: \n",
            "Hidden Size: 100\n",
            "Batch Size: 1\n",
            "Learning Rate: 0.001\n",
            "Num Epochs: 6\n",
            "Num Topics; 150\n",
            "\n",
            "PR-AUC: 0.6455\n",
            "------------------\n",
            "\n",
            "Current parameters: \n",
            "Hidden Size: 100\n",
            "Batch Size: 1\n",
            "Learning Rate: 0.005\n",
            "Num Epochs: 2\n",
            "Num Topics; 50\n",
            "\n",
            "PR-AUC: 0.6389\n",
            "------------------\n",
            "\n",
            "Current parameters: \n",
            "Hidden Size: 100\n",
            "Batch Size: 1\n",
            "Learning Rate: 0.005\n",
            "Num Epochs: 2\n",
            "Num Topics; 150\n",
            "\n",
            "PR-AUC: 0.6300\n",
            "------------------\n",
            "\n",
            "Current parameters: \n",
            "Hidden Size: 100\n",
            "Batch Size: 1\n",
            "Learning Rate: 0.005\n",
            "Num Epochs: 6\n",
            "Num Topics; 50\n",
            "\n",
            "PR-AUC: 0.6399\n",
            "------------------\n",
            "\n",
            "Current parameters: \n",
            "Hidden Size: 100\n",
            "Batch Size: 1\n",
            "Learning Rate: 0.005\n",
            "Num Epochs: 6\n",
            "Num Topics; 150\n",
            "\n",
            "PR-AUC: 0.6413\n",
            "------------------\n",
            "\n",
            "Current parameters: \n",
            "Hidden Size: 100\n",
            "Batch Size: 1\n",
            "Learning Rate: 0.001\n",
            "Num Epochs: 2\n",
            "Num Topics; 50\n",
            "\n",
            "PR-AUC: 0.6436\n",
            "------------------\n",
            "\n",
            "Current parameters: \n",
            "Hidden Size: 100\n",
            "Batch Size: 1\n",
            "Learning Rate: 0.001\n",
            "Num Epochs: 2\n",
            "Num Topics; 150\n",
            "\n",
            "PR-AUC: 0.6454\n",
            "------------------\n",
            "\n",
            "Current parameters: \n",
            "Hidden Size: 100\n",
            "Batch Size: 1\n",
            "Learning Rate: 0.001\n",
            "Num Epochs: 6\n",
            "Num Topics; 50\n",
            "\n",
            "PR-AUC: 0.6446\n",
            "------------------\n",
            "\n",
            "Current parameters: \n",
            "Hidden Size: 100\n",
            "Batch Size: 1\n",
            "Learning Rate: 0.001\n",
            "Num Epochs: 6\n",
            "Num Topics; 150\n",
            "\n",
            "PR-AUC: 0.6416\n",
            "------------------\n",
            "\n",
            "Current parameters: \n",
            "Hidden Size: 100\n",
            "Batch Size: 1\n",
            "Learning Rate: 0.005\n",
            "Num Epochs: 2\n",
            "Num Topics; 50\n",
            "\n",
            "PR-AUC: 0.6423\n",
            "------------------\n",
            "\n",
            "Current parameters: \n",
            "Hidden Size: 100\n",
            "Batch Size: 1\n",
            "Learning Rate: 0.005\n",
            "Num Epochs: 2\n",
            "Num Topics; 150\n",
            "\n",
            "PR-AUC: 0.6429\n",
            "------------------\n",
            "\n",
            "Current parameters: \n",
            "Hidden Size: 100\n",
            "Batch Size: 1\n",
            "Learning Rate: 0.005\n",
            "Num Epochs: 6\n",
            "Num Topics; 50\n",
            "\n",
            "PR-AUC: 0.6428\n",
            "------------------\n",
            "\n",
            "Current parameters: \n",
            "Hidden Size: 100\n",
            "Batch Size: 1\n",
            "Learning Rate: 0.005\n",
            "Num Epochs: 6\n",
            "Num Topics; 150\n",
            "\n",
            "PR-AUC: 0.6392\n",
            "------------------\n",
            "\n",
            "Best validation PR-AUC: 0.6454028876871399\n",
            "Best hyperparameters: {'hidden_size': 100, 'batch_size': 1, 'learning_rate': 0.001, 'num_epochs': 2, 'n_topics': 150}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import itertools\n",
        "\n",
        "# DEFINE HYPERPARAMETERS YOU WANT TO TRY HERE\n",
        "param_grid = {\n",
        "    \"hidden_size\": [100],\n",
        "    \"batch_size\": [1],\n",
        "    \"learning_rate\": [0.001, 0.005],\n",
        "    \"num_epochs\": [2, 6],\n",
        "    \"n_topics\": [50, 150]\n",
        "}\n",
        "\n",
        "# Run grid search on CONTENT\n",
        "best_val_metric, best_config = grid_search(param_grid, ContentModel)\n",
        "\n",
        "# Run grid search on GRU\n",
        "best_val_metric, best_config = grid_search(param_grid, GRUModel)\n",
        "\n",
        "# Print results\n",
        "print(\"Best validation PR-AUC:\", best_val_metric)\n",
        "print(\"Best hyperparameters:\", best_config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cGGbhitmbLF"
      },
      "source": [
        "### Update Config to Optimized Hyperparameters for Final Training + Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiPruE2UMBCo"
      },
      "source": [
        "This code block updates the Config so that the paper parameters are updated to the optimized parameters we found using the Grid Search function above. Since this is an extention to the original replication, we are commenting it out. As a result, the model will be trained and evaluated using the original paper's parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyIK_XZnml8F"
      },
      "outputs": [],
      "source": [
        "# ---- If you want to use Original Paper Parameters, DO NOT RUN THIS!! ----\n",
        "# this code block updates the config so that the paper parameters are updated to\n",
        "# the optimized parameters found using the Grid Search function above\n",
        "\n",
        "#config.embed_size = best_config['embed_size']\n",
        "#config.hidden_size = best_config['hidden_size']\n",
        "#config.batch_size = best_config['batch_size']\n",
        "#config.learning_rate = best_config['learning_rate']\n",
        "#config.num_epochs = best_config['num_epochs']\n",
        "\n",
        "# (Re)create your Training and Testing DataLoaders with the updated batch size if necessary\n",
        "# train_loader = create_dataloader(X_train, Y_train, config, shuffle=True)\n",
        "# test_loader = create_dataloader(X_test,  Y_test,  config, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0HrzmLHM-xU"
      },
      "source": [
        "For reference, here are the optimal parameters we found using our Grid Search function.\n",
        "\n",
        "Optimal hyperparams we found for CONTENT:\n",
        "\n",
        "* hidden_size = 100\n",
        "* batch_size = 1\n",
        "* learning_rate = 0.001\n",
        "* num_epochs = 2\n",
        "* n_topics = 150\n",
        "\n",
        "Optimal hyperparams we found for GRU:\n",
        "* hidden_size = 100\n",
        "* batch_size = 1\n",
        "* learning_rate = 0.005\n",
        "* num_epochs = 2\n",
        "* n_topics = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZz74tMTvKQS"
      },
      "source": [
        "## Train and Test Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAW8MLh6uqCn"
      },
      "source": [
        "### Defining Model Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL8r7UEZD1Lm"
      },
      "source": [
        "#### CONTENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sb9Hy7E1utKu"
      },
      "outputs": [],
      "source": [
        "def init_content():\n",
        "  # Initialize CONTENT with the hardcoded/grid-search hyperparam\n",
        "  content_model = ContentModel(config).to(config.device)\n",
        "  content_optimizer = torch.optim.Adam(content_model.parameters(), lr=config.learning_rate)\n",
        "  return content_model, content_optimizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEMqHoomD3bJ"
      },
      "source": [
        "#### GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRvuhojwD4u-"
      },
      "outputs": [],
      "source": [
        "def init_gru():\n",
        "  # GRU model\n",
        "  gru_model = GRUModel(config).to(config.device)\n",
        "  gru_optimizer = torch.optim.Adam(gru_model.parameters(), lr=config.learning_rate)\n",
        "  return gru_model, gru_optimizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfy1_eor2R6Z"
      },
      "source": [
        "### Word2Vec Embedding (CONTENT model only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HT-14qxd2azf",
        "outputId": "81cfa82d-3f23-4aba-86ba-f2c2aa7e20d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: ../resource/vocab.pkl\n",
            "Vocab size: 490\n"
          ]
        }
      ],
      "source": [
        "# Initialize weights with embedded matrix (CONTENT model only)\n",
        "wordvecPath = os.path.join(config.dataset_dir, \"word2vec.vector\")\n",
        "vocab_mapping = load_vocab_dict(config)\n",
        "W_embed = loadEmbeddingMatrix(wordvecPath, vocab_mapping, config.vocab_size)\n",
        "\n",
        "def word2vec(content_model):\n",
        "  with torch.no_grad():\n",
        "      content_model.embed.weight.copy_(torch.from_numpy(W_embed.T).to(config.device))\n",
        "  return content_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rCD7WZmrCAF"
      },
      "source": [
        "### Defining Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjMo69qhEBuJ"
      },
      "source": [
        "#### CONTENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_O8ndEhbrAAw"
      },
      "outputs": [],
      "source": [
        "def train_content(content_model, content_optimizer):\n",
        "  # Record training losses\n",
        "  content_model_losses = []\n",
        "\n",
        "  # CONTENT model training\n",
        "  print(\"Training CONTENT model:\")\n",
        "  for epoch in range(config.num_epochs):\n",
        "    st = time.time()\n",
        "    train_loss, train_theta_collector = train(content_model, train_loader, content_optimizer, config)\n",
        "\n",
        "    # Append current training loss to list\n",
        "    content_model_losses.append(train_loss)\n",
        "\n",
        "    # Save theta outputs for the epoch\n",
        "    train_thetas_arr = np.concatenate(train_theta_collector, axis=0)\n",
        "    np.save(os.path.join(config.content_theta_dir, f\"content_thetas_train_{epoch}.npy\"), train_thetas_arr)\n",
        "\n",
        "    elapsed = time.time() - st\n",
        "    print(f\"\\nEpoch {epoch+1}/{config.num_epochs} took {elapsed:.2f}s\")\n",
        "    print(f\"  [Train] loss={train_loss:.4f}\")\n",
        "\n",
        "  return content_model_losses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KT3KREkEGEM"
      },
      "source": [
        "#### GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qftKioKEH2s"
      },
      "outputs": [],
      "source": [
        "def train_gru(gru_model, gru_optimizer):\n",
        "  # Record training losses\n",
        "  gru_model_losses = []\n",
        "\n",
        "  # GRU model training\n",
        "  print(\"Training GRU model:\")\n",
        "  for epoch in range(config.num_epochs):\n",
        "    st = time.time()\n",
        "    train_loss, train_hidden_collector = train(gru_model, train_loader, gru_optimizer, config)\n",
        "\n",
        "    # Append current training loss to list\n",
        "    gru_model_losses.append(train_loss)\n",
        "\n",
        "    # Save hidden outputs for the epoch\n",
        "    train_hidden_arr = np.concatenate(train_hidden_collector, axis=0)\n",
        "    np.save(os.path.join(config.gru_hiddens_dir, f\"gru_hiddens_train_{epoch}.npy\"), train_hidden_arr)\n",
        "\n",
        "    elapsed = time.time() - st\n",
        "    print(f\"\\nEpoch {epoch+1}/{config.num_epochs} took {elapsed:.2f}s\")\n",
        "    print(f\"  [Train] loss={train_loss:.4f}\")\n",
        "\n",
        "  return gru_model_losses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9KZthGewh91"
      },
      "source": [
        "### Defining Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPsSWx1MXiEg"
      },
      "source": [
        "#### CONTENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bafMZ9UUwnFE"
      },
      "outputs": [],
      "source": [
        "def plot_content_loss(content_model_losses):\n",
        "  # Epochs for the x-axis\n",
        "  epochs = range(1, config.num_epochs + 1)\n",
        "\n",
        "  # Plot\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(epochs, content_model_losses, label='CONTENT Model', marker='o')\n",
        "\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Training Loss')\n",
        "  plt.title('Training Loss over Epochs')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.tight_layout()\n",
        "\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whtcjcmfXj83"
      },
      "source": [
        "#### GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_WsrSxnTm-f"
      },
      "outputs": [],
      "source": [
        "def plot_gru_loss(gru_model_losses):\n",
        "  # Epochs for the x-axis\n",
        "  epochs = range(1, config.num_epochs + 1)\n",
        "\n",
        "  # Plot\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(epochs, gru_model_losses, label='GRU Model', marker='o')\n",
        "\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Training Loss')\n",
        "  plt.title('Training Loss over Epochs')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.tight_layout()\n",
        "\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ub5jcT6Mefqb"
      },
      "source": [
        "### Defining Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxMOP3awE6m3"
      },
      "source": [
        "#### CONTENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DevIGIDCdx79"
      },
      "outputs": [],
      "source": [
        "def test_content(content_model):\n",
        "  # Evaluate CONTENT model on Testing Data\n",
        "  content_test_loss, content_test_true, content_test_pred, content_test_theta_collector = evaluate_model(content_model, test_loader, config)\n",
        "  content_test_metrics = compute_metrics(content_test_true, content_test_pred)\n",
        "\n",
        "  # Save the CONTENT test results\n",
        "  content_test_thetas_arr = np.concatenate(content_test_theta_collector, axis=0)\n",
        "  np.save(os.path.join(config.content_theta_dir, f\"content_thetas_test_final.npy\"), content_test_thetas_arr)\n",
        "  np.save(os.path.join(config.content_results_dir, f\"content_test_labels_final.npy\"), np.array(content_test_true))\n",
        "  np.save(os.path.join(config.content_results_dir, f\"content_test_preds_final.npy\"), np.array(content_test_pred))\n",
        "\n",
        "  print(f\"\\n[CONTENT Test] loss={content_test_loss:.4f}, AUC={content_test_metrics['auc']:.4f}, \"\n",
        "        f\"PR-AUC={content_test_metrics['prauc']:.4f}, ACC={content_test_metrics['acc']:.4f}, \"\n",
        "        f\"Precision={content_test_metrics['precision']:.4f}, Recall={content_test_metrics['recall']:.4f}, \"\n",
        "        f\"F1={content_test_metrics['f1']:.4f}\")\n",
        "\n",
        "  return content_test_metrics['auc'], content_test_metrics['prauc'], content_test_metrics['acc'], content_test_metrics['precision'], content_test_metrics['recall'], content_test_metrics['f1']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7k0q0n_yEzKT"
      },
      "source": [
        "#### GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-qpqa2qEzki"
      },
      "outputs": [],
      "source": [
        "def test_gru(gru_model):\n",
        "  # Evaluate GRU model on Testing Data\n",
        "  gru_test_loss, gru_test_true, gru_test_pred, gru_test_hidden_collector = evaluate_model(gru_model, test_loader, config)\n",
        "  gru_test_metrics = compute_metrics(gru_test_true, gru_test_pred)\n",
        "\n",
        "  # Save the GRU test results\n",
        "  gru_test_hidden_arr = np.concatenate(gru_test_hidden_collector, axis=0)\n",
        "  np.save(os.path.join(config.gru_hiddens_dir, f\"gru_hiddens_test_final.npy\"), gru_test_hidden_arr)\n",
        "  np.save(os.path.join(config.gru_results_dir, f\"gru_test_labels_final.npy\"), np.array(gru_test_true))\n",
        "  np.save(os.path.join(config.gru_results_dir, f\"gru_test_preds_final.npy\"), np.array(gru_test_pred))\n",
        "\n",
        "  print(f\"\\n[GRU Test] loss={gru_test_loss:.4f}, AUC={gru_test_metrics['auc']:.4f}, \"\n",
        "        f\"PR-AUC={gru_test_metrics['prauc']:.4f}, ACC={gru_test_metrics['acc']:.4f}, \"\n",
        "        f\"Precision={gru_test_metrics['precision']:.4f}, Recall={gru_test_metrics['recall']:.4f}, \"\n",
        "        f\"F1={gru_test_metrics['f1']:.4f}\")\n",
        "\n",
        "  return gru_test_metrics['auc'], gru_test_metrics['prauc'], gru_test_metrics['acc'], gru_test_metrics['precision'], gru_test_metrics['recall'], gru_test_metrics['f1']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnswxLOnCbFc"
      },
      "source": [
        "### Executing Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsRJZQUUFGG8"
      },
      "source": [
        "#### CONTENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiK3PkRLKdnH",
        "outputId": "f29a570f-c271-4fdf-9ba4-1f35c0538f90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embed Size: 100\n",
            "Hidden Size: 200\n",
            "Batch Size: 1\n",
            "Learning Rate: 0.001\n",
            "Num Topics: 50\n",
            "Num Epochs: 6\n",
            "Num Trials: 10\n"
          ]
        }
      ],
      "source": [
        "# Optional, confirm which params you are using\n",
        "print(f\"Embed Size: {config.embed_size}\")\n",
        "print(f\"Hidden Size: {config.hidden_size}\")\n",
        "print(f\"Batch Size: {config.batch_size}\")\n",
        "print(f\"Learning Rate: {config.learning_rate}\")\n",
        "print(f\"Num Topics: {config.n_topics}\")\n",
        "print(f\"Num Epochs: {config.num_epochs}\")\n",
        "print(f\"Num Trials: {config.num_trials}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNUbZW4-Q-u1",
        "outputId": "229d7382-7753-4906-d4a5-8b376947185c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----TRIAL 1:-----\n",
            "Training CONTENT model:\n",
            "\n",
            "Epoch 1/6 took 23.83s\n",
            "  [Train] loss=0.4170\n",
            "\n",
            "Epoch 2/6 took 30.39s\n",
            "  [Train] loss=0.4002\n",
            "\n",
            "Epoch 3/6 took 25.83s\n",
            "  [Train] loss=0.3916\n",
            "\n",
            "Epoch 4/6 took 28.47s\n",
            "  [Train] loss=0.3801\n",
            "\n",
            "Epoch 5/6 took 26.46s\n",
            "  [Train] loss=0.3663\n",
            "\n",
            "Epoch 6/6 took 27.75s\n",
            "  [Train] loss=0.3507\n",
            "\n",
            "[CONTENT Test] loss=0.3996, AUC=0.7932, PR-AUC=0.6387, ACC=0.8353, Precision=0.7318, Recall=0.4169, F1=0.5312\n",
            "------------------\n",
            "\n",
            "-----TRIAL 2:-----\n",
            "Training CONTENT model:\n",
            "\n",
            "Epoch 1/6 took 23.10s\n",
            "  [Train] loss=0.4167\n",
            "\n",
            "Epoch 2/6 took 26.40s\n",
            "  [Train] loss=0.4010\n",
            "\n",
            "Epoch 3/6 took 22.44s\n",
            "  [Train] loss=0.3908\n",
            "\n",
            "Epoch 4/6 took 21.56s\n",
            "  [Train] loss=0.3812\n",
            "\n",
            "Epoch 5/6 took 27.95s\n",
            "  [Train] loss=0.3669\n",
            "\n",
            "Epoch 6/6 took 25.63s\n",
            "  [Train] loss=0.3515\n",
            "\n",
            "[CONTENT Test] loss=0.4081, AUC=0.7911, PR-AUC=0.6350, ACC=0.8344, Precision=0.7915, Recall=0.3534, F1=0.4886\n",
            "------------------\n",
            "\n",
            "-----TRIAL 3:-----\n",
            "Training CONTENT model:\n",
            "\n",
            "Epoch 1/6 took 31.39s\n",
            "  [Train] loss=0.4162\n",
            "\n",
            "Epoch 2/6 took 28.93s\n",
            "  [Train] loss=0.4004\n",
            "\n",
            "Epoch 3/6 took 26.25s\n",
            "  [Train] loss=0.3916\n",
            "\n",
            "Epoch 4/6 took 22.70s\n",
            "  [Train] loss=0.3804\n",
            "\n",
            "Epoch 5/6 took 21.61s\n",
            "  [Train] loss=0.3668\n",
            "\n",
            "Epoch 6/6 took 22.42s\n",
            "  [Train] loss=0.3515\n",
            "\n",
            "[CONTENT Test] loss=0.4061, AUC=0.7912, PR-AUC=0.6356, ACC=0.8316, Precision=0.6924, Recall=0.4455, F1=0.5421\n",
            "------------------\n",
            "\n",
            "-----TRIAL 4:-----\n",
            "Training CONTENT model:\n",
            "\n",
            "Epoch 1/6 took 22.17s\n",
            "  [Train] loss=0.4166\n",
            "\n",
            "Epoch 2/6 took 25.83s\n",
            "  [Train] loss=0.4010\n",
            "\n",
            "Epoch 3/6 took 22.90s\n",
            "  [Train] loss=0.3916\n",
            "\n",
            "Epoch 4/6 took 21.58s\n",
            "  [Train] loss=0.3819\n",
            "\n",
            "Epoch 5/6 took 22.48s\n",
            "  [Train] loss=0.3675\n",
            "\n",
            "Epoch 6/6 took 22.46s\n",
            "  [Train] loss=0.3525\n",
            "\n",
            "[CONTENT Test] loss=0.4100, AUC=0.7932, PR-AUC=0.6377, ACC=0.8269, Precision=0.6504, Recall=0.4894, F1=0.5585\n",
            "------------------\n",
            "\n",
            "-----TRIAL 5:-----\n",
            "Training CONTENT model:\n",
            "\n",
            "Epoch 1/6 took 22.08s\n",
            "  [Train] loss=0.4170\n",
            "\n",
            "Epoch 2/6 took 21.50s\n",
            "  [Train] loss=0.4011\n",
            "\n",
            "Epoch 3/6 took 22.23s\n",
            "  [Train] loss=0.3913\n",
            "\n",
            "Epoch 4/6 took 22.36s\n",
            "  [Train] loss=0.3815\n",
            "\n",
            "Epoch 5/6 took 22.21s\n",
            "  [Train] loss=0.3679\n",
            "\n",
            "Epoch 6/6 took 21.83s\n",
            "  [Train] loss=0.3529\n",
            "\n",
            "[CONTENT Test] loss=0.4073, AUC=0.7944, PR-AUC=0.6411, ACC=0.8297, Precision=0.6688, Recall=0.4735, F1=0.5544\n",
            "------------------\n",
            "\n",
            "-----TRIAL 6:-----\n",
            "Training CONTENT model:\n",
            "\n",
            "Epoch 1/6 took 21.87s\n",
            "  [Train] loss=0.4166\n",
            "\n",
            "Epoch 2/6 took 22.28s\n",
            "  [Train] loss=0.4003\n",
            "\n",
            "Epoch 3/6 took 22.25s\n",
            "  [Train] loss=0.3916\n",
            "\n",
            "Epoch 4/6 took 21.77s\n",
            "  [Train] loss=0.3804\n",
            "\n",
            "Epoch 5/6 took 22.29s\n",
            "  [Train] loss=0.3672\n",
            "\n",
            "Epoch 6/6 took 22.40s\n",
            "  [Train] loss=0.3518\n",
            "\n",
            "[CONTENT Test] loss=0.4027, AUC=0.7925, PR-AUC=0.6369, ACC=0.8348, Precision=0.7276, Recall=0.4183, F1=0.5312\n",
            "------------------\n",
            "\n",
            "-----TRIAL 7:-----\n",
            "Training CONTENT model:\n",
            "\n",
            "Epoch 1/6 took 22.19s\n",
            "  [Train] loss=0.4181\n",
            "\n",
            "Epoch 2/6 took 21.43s\n",
            "  [Train] loss=0.4000\n",
            "\n",
            "Epoch 3/6 took 22.28s\n",
            "  [Train] loss=0.3910\n",
            "\n",
            "Epoch 4/6 took 22.37s\n",
            "  [Train] loss=0.3807\n",
            "\n",
            "Epoch 5/6 took 21.79s\n",
            "  [Train] loss=0.3676\n",
            "\n",
            "Epoch 6/6 took 22.21s\n",
            "  [Train] loss=0.3504\n",
            "\n",
            "[CONTENT Test] loss=0.4051, AUC=0.7937, PR-AUC=0.6378, ACC=0.8317, Precision=0.6849, Recall=0.4595, F1=0.5500\n",
            "------------------\n",
            "\n",
            "-----TRIAL 8:-----\n",
            "Training CONTENT model:\n",
            "\n",
            "Epoch 1/6 took 22.10s\n",
            "  [Train] loss=0.4171\n",
            "\n",
            "Epoch 2/6 took 22.26s\n",
            "  [Train] loss=0.4001\n",
            "\n",
            "Epoch 3/6 took 21.78s\n",
            "  [Train] loss=0.3914\n",
            "\n",
            "Epoch 4/6 took 22.00s\n",
            "  [Train] loss=0.3799\n",
            "\n",
            "Epoch 5/6 took 22.38s\n",
            "  [Train] loss=0.3659\n",
            "\n",
            "Epoch 6/6 took 22.61s\n",
            "  [Train] loss=0.3496\n",
            "\n",
            "[CONTENT Test] loss=0.4092, AUC=0.7902, PR-AUC=0.6312, ACC=0.8313, Precision=0.7051, Recall=0.4233, F1=0.5290\n",
            "------------------\n",
            "\n",
            "-----TRIAL 9:-----\n",
            "Training CONTENT model:\n",
            "\n",
            "Epoch 1/6 took 21.84s\n",
            "  [Train] loss=0.4165\n",
            "\n",
            "Epoch 2/6 took 21.82s\n",
            "  [Train] loss=0.4001\n",
            "\n",
            "Epoch 3/6 took 22.27s\n",
            "  [Train] loss=0.3918\n",
            "\n",
            "Epoch 4/6 took 22.41s\n",
            "  [Train] loss=0.3821\n",
            "\n",
            "Epoch 5/6 took 21.64s\n",
            "  [Train] loss=0.3695\n",
            "\n",
            "Epoch 6/6 took 22.35s\n",
            "  [Train] loss=0.3547\n",
            "\n",
            "[CONTENT Test] loss=0.4087, AUC=0.7935, PR-AUC=0.6367, ACC=0.8309, Precision=0.6759, Recall=0.4693, F1=0.5540\n",
            "------------------\n",
            "\n",
            "-----TRIAL 10:-----\n",
            "Training CONTENT model:\n",
            "\n",
            "Epoch 1/6 took 22.08s\n",
            "  [Train] loss=0.4174\n",
            "\n",
            "Epoch 2/6 took 22.25s\n",
            "  [Train] loss=0.4001\n",
            "\n",
            "Epoch 3/6 took 21.49s\n",
            "  [Train] loss=0.3922\n",
            "\n",
            "Epoch 4/6 took 22.24s\n",
            "  [Train] loss=0.3803\n",
            "\n",
            "Epoch 5/6 took 22.45s\n",
            "  [Train] loss=0.3676\n",
            "\n",
            "Epoch 6/6 took 22.21s\n",
            "  [Train] loss=0.3530\n",
            "\n",
            "[CONTENT Test] loss=0.3995, AUC=0.7961, PR-AUC=0.6377, ACC=0.8348, Precision=0.7292, Recall=0.4163, F1=0.5300\n",
            "------------------\n",
            "\n",
            "\n",
            "[CONTENT RESULTS OVER TRIALS] AUC = 0.7929 +/- 0.0017, PRAUC = 0.6368 +/- 0.0026, ACC = 0.8321 +/- 0.0027, PRECISION = 0.7058 +/- 0.0408, RECALL = 0.4365 +/- 0.0395, F1 = 0.5369 +/- 0.0205\n"
          ]
        }
      ],
      "source": [
        "def run_content(trial, seed=None):\n",
        "  print(f\"-----TRIAL {trial}:-----\")\n",
        "  # Use new seed\n",
        "  if seed is not None:\n",
        "      torch.manual_seed(seed)\n",
        "      np.random.seed(seed)\n",
        "\n",
        "  # Brand‑new model & optimiser\n",
        "  content_model, content_optimizer = init_content()\n",
        "\n",
        "  # Optional, but part of original CONTENT: word2vec embedding initialization\n",
        "  content_model = word2vec(content_model)\n",
        "\n",
        "  # Train it\n",
        "  content_model_losses = train_content(content_model, content_optimizer)\n",
        "\n",
        "  # Optional: plot training loss\n",
        "  # plot_content_loss(content_model_losses)\n",
        "\n",
        "  # Evaluate and collect metrics\n",
        "  metrics = test_content(content_model)\n",
        "  print(\"------------------\\n\")\n",
        "  return metrics\n",
        "\n",
        "# Repeat\n",
        "content_metrics = [run_content(i + 1, seed=i) for i in range(config.num_trials)]\n",
        "content_aucs, content_praucs, content_accs, \\\n",
        "content_precisions, content_recalls, content_f1s = map(list, zip(*content_metrics))\n",
        "\n",
        "\n",
        "print(\n",
        "    \"\\n[CONTENT RESULTS OVER TRIALS] \"\n",
        "    f\"AUC = {round(mean(content_aucs), 4)} +/- {round(stdev(content_aucs), 4)}, \"\n",
        "    f\"PRAUC = {round(mean(content_praucs), 4)} +/- {round(stdev(content_praucs), 4)}, \"\n",
        "    f\"ACC = {round(mean(content_accs), 4)} +/- {round(stdev(content_accs), 4)}, \"\n",
        "    f\"PRECISION = {round(mean(content_precisions), 4)} +/- {round(stdev(content_precisions), 4)}, \"\n",
        "    f\"RECALL = {round(mean(content_recalls), 4)} +/- {round(stdev(content_recalls), 4)}, \"\n",
        "    f\"F1 = {round(mean(content_f1s), 4)} +/- {round(stdev(content_f1s), 4)}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2_TzF5qFIOC"
      },
      "source": [
        "#### GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aw_hvD1ECaCy",
        "outputId": "5d491347-cd02-4cb9-d081-75f18920df44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----TRIAL 1:-----\n",
            "Training GRU model:\n",
            "\n",
            "Epoch 1/6 took 18.50s\n",
            "  [Train] loss=0.4286\n",
            "\n",
            "Epoch 2/6 took 17.94s\n",
            "  [Train] loss=0.4017\n",
            "\n",
            "Epoch 3/6 took 18.56s\n",
            "  [Train] loss=0.3956\n",
            "\n",
            "Epoch 4/6 took 17.78s\n",
            "  [Train] loss=0.3872\n",
            "\n",
            "Epoch 5/6 took 18.11s\n",
            "  [Train] loss=0.3772\n",
            "\n",
            "Epoch 6/6 took 18.35s\n",
            "  [Train] loss=0.3647\n",
            "\n",
            "[GRU Test] loss=0.4045, AUC=0.7915, PR-AUC=0.6337, ACC=0.8332, Precision=0.7243, Recall=0.4116, F1=0.5249\n",
            "------------------\n",
            "\n",
            "-----TRIAL 2:-----\n",
            "Training GRU model:\n",
            "\n",
            "Epoch 1/6 took 17.76s\n",
            "  [Train] loss=0.4273\n",
            "\n",
            "Epoch 2/6 took 17.58s\n",
            "  [Train] loss=0.4019\n",
            "\n",
            "Epoch 3/6 took 17.37s\n",
            "  [Train] loss=0.3953\n",
            "\n",
            "Epoch 4/6 took 18.07s\n",
            "  [Train] loss=0.3875\n",
            "\n",
            "Epoch 5/6 took 17.46s\n",
            "  [Train] loss=0.3776\n",
            "\n",
            "Epoch 6/6 took 17.37s\n",
            "  [Train] loss=0.3656\n",
            "\n",
            "[GRU Test] loss=0.4041, AUC=0.7891, PR-AUC=0.6341, ACC=0.8336, Precision=0.7127, Recall=0.4296, F1=0.5361\n",
            "------------------\n",
            "\n",
            "-----TRIAL 3:-----\n",
            "Training GRU model:\n",
            "\n",
            "Epoch 1/6 took 17.71s\n",
            "  [Train] loss=0.4285\n",
            "\n",
            "Epoch 2/6 took 17.35s\n",
            "  [Train] loss=0.4023\n",
            "\n",
            "Epoch 3/6 took 18.06s\n",
            "  [Train] loss=0.3951\n",
            "\n",
            "Epoch 4/6 took 17.43s\n",
            "  [Train] loss=0.3869\n",
            "\n",
            "Epoch 5/6 took 17.46s\n",
            "  [Train] loss=0.3764\n",
            "\n",
            "Epoch 6/6 took 18.15s\n",
            "  [Train] loss=0.3641\n",
            "\n",
            "[GRU Test] loss=0.4093, AUC=0.7873, PR-AUC=0.6255, ACC=0.8291, Precision=0.6888, Recall=0.4310, F1=0.5302\n",
            "------------------\n",
            "\n",
            "-----TRIAL 4:-----\n",
            "Training GRU model:\n",
            "\n",
            "Epoch 1/6 took 17.27s\n",
            "  [Train] loss=0.4280\n",
            "\n",
            "Epoch 2/6 took 17.96s\n",
            "  [Train] loss=0.4021\n",
            "\n",
            "Epoch 3/6 took 17.42s\n",
            "  [Train] loss=0.3963\n",
            "\n",
            "Epoch 4/6 took 17.30s\n",
            "  [Train] loss=0.3881\n",
            "\n",
            "Epoch 5/6 took 18.08s\n",
            "  [Train] loss=0.3773\n",
            "\n",
            "Epoch 6/6 took 17.31s\n",
            "  [Train] loss=0.3662\n",
            "\n",
            "[GRU Test] loss=0.4101, AUC=0.7841, PR-AUC=0.6240, ACC=0.8303, Precision=0.7027, Recall=0.4195, F1=0.5253\n",
            "------------------\n",
            "\n",
            "-----TRIAL 5:-----\n",
            "Training GRU model:\n",
            "\n",
            "Epoch 1/6 took 17.77s\n",
            "  [Train] loss=0.4272\n",
            "\n",
            "Epoch 2/6 took 17.58s\n",
            "  [Train] loss=0.4021\n",
            "\n",
            "Epoch 3/6 took 17.36s\n",
            "  [Train] loss=0.3970\n",
            "\n",
            "Epoch 4/6 took 18.10s\n",
            "  [Train] loss=0.3880\n",
            "\n",
            "Epoch 5/6 took 17.37s\n",
            "  [Train] loss=0.3780\n",
            "\n",
            "Epoch 6/6 took 17.28s\n",
            "  [Train] loss=0.3673\n",
            "\n",
            "[GRU Test] loss=0.4095, AUC=0.7888, PR-AUC=0.6329, ACC=0.8299, Precision=0.6752, Recall=0.4623, F1=0.5488\n",
            "------------------\n",
            "\n",
            "-----TRIAL 6:-----\n",
            "Training GRU model:\n",
            "\n",
            "Epoch 1/6 took 17.66s\n",
            "  [Train] loss=0.4274\n",
            "\n",
            "Epoch 2/6 took 17.28s\n",
            "  [Train] loss=0.4012\n",
            "\n",
            "Epoch 3/6 took 18.05s\n",
            "  [Train] loss=0.3953\n",
            "\n",
            "Epoch 4/6 took 17.47s\n",
            "  [Train] loss=0.3869\n",
            "\n",
            "Epoch 5/6 took 17.35s\n",
            "  [Train] loss=0.3760\n",
            "\n",
            "Epoch 6/6 took 18.17s\n",
            "  [Train] loss=0.3645\n",
            "\n",
            "[GRU Test] loss=0.4073, AUC=0.7859, PR-AUC=0.6259, ACC=0.8302, Precision=0.6967, Recall=0.4271, F1=0.5296\n",
            "------------------\n",
            "\n",
            "-----TRIAL 7:-----\n",
            "Training GRU model:\n",
            "\n",
            "Epoch 1/6 took 17.28s\n",
            "  [Train] loss=0.4259\n",
            "\n",
            "Epoch 2/6 took 17.97s\n",
            "  [Train] loss=0.4026\n",
            "\n",
            "Epoch 3/6 took 17.52s\n",
            "  [Train] loss=0.3961\n",
            "\n",
            "Epoch 4/6 took 17.30s\n",
            "  [Train] loss=0.3885\n",
            "\n",
            "Epoch 5/6 took 18.16s\n",
            "  [Train] loss=0.3802\n",
            "\n",
            "Epoch 6/6 took 17.33s\n",
            "  [Train] loss=0.3691\n",
            "\n",
            "[GRU Test] loss=0.4006, AUC=0.7898, PR-AUC=0.6341, ACC=0.8345, Precision=0.7479, Recall=0.3931, F1=0.5153\n",
            "------------------\n",
            "\n",
            "-----TRIAL 8:-----\n",
            "Training GRU model:\n",
            "\n",
            "Epoch 1/6 took 17.68s\n",
            "  [Train] loss=0.4286\n",
            "\n",
            "Epoch 2/6 took 17.59s\n",
            "  [Train] loss=0.4027\n",
            "\n",
            "Epoch 3/6 took 17.36s\n",
            "  [Train] loss=0.3962\n",
            "\n",
            "Epoch 4/6 took 18.10s\n",
            "  [Train] loss=0.3868\n",
            "\n",
            "Epoch 5/6 took 17.40s\n",
            "  [Train] loss=0.3768\n",
            "\n",
            "Epoch 6/6 took 17.31s\n",
            "  [Train] loss=0.3641\n",
            "\n",
            "[GRU Test] loss=0.4044, AUC=0.7878, PR-AUC=0.6287, ACC=0.8325, Precision=0.7207, Recall=0.4106, F1=0.5231\n",
            "------------------\n",
            "\n",
            "-----TRIAL 9:-----\n",
            "Training GRU model:\n",
            "\n",
            "Epoch 1/6 took 17.70s\n",
            "  [Train] loss=0.4271\n",
            "\n",
            "Epoch 2/6 took 17.23s\n",
            "  [Train] loss=0.4023\n",
            "\n",
            "Epoch 3/6 took 18.28s\n",
            "  [Train] loss=0.3950\n",
            "\n",
            "Epoch 4/6 took 17.43s\n",
            "  [Train] loss=0.3871\n",
            "\n",
            "Epoch 5/6 took 17.51s\n",
            "  [Train] loss=0.3765\n",
            "\n",
            "Epoch 6/6 took 18.33s\n",
            "  [Train] loss=0.3635\n",
            "\n",
            "[GRU Test] loss=0.4108, AUC=0.7880, PR-AUC=0.6289, ACC=0.8266, Precision=0.6635, Recall=0.4565, F1=0.5409\n",
            "------------------\n",
            "\n",
            "-----TRIAL 10:-----\n",
            "Training GRU model:\n",
            "\n",
            "Epoch 1/6 took 17.29s\n",
            "  [Train] loss=0.4293\n",
            "\n",
            "Epoch 2/6 took 18.06s\n",
            "  [Train] loss=0.4019\n",
            "\n",
            "Epoch 3/6 took 17.98s\n",
            "  [Train] loss=0.3958\n",
            "\n",
            "Epoch 4/6 took 17.50s\n",
            "  [Train] loss=0.3885\n",
            "\n",
            "Epoch 5/6 took 18.45s\n",
            "  [Train] loss=0.3780\n",
            "\n",
            "Epoch 6/6 took 17.46s\n",
            "  [Train] loss=0.3665\n",
            "\n",
            "[GRU Test] loss=0.4035, AUC=0.7868, PR-AUC=0.6270, ACC=0.8330, Precision=0.7459, Recall=0.3850, F1=0.5079\n",
            "------------------\n",
            "\n",
            "\n",
            "[GRU RESULTS OVER TRIALS] AUC = 0.7879 +/- 0.0021, PRAUC = 0.6295 +/- 0.0039, ACC = 0.8313 +/- 0.0025, PRECISION = 0.7079 +/- 0.028, RECALL = 0.4226 +/- 0.0246, F1 = 0.5282 +/- 0.0119\n"
          ]
        }
      ],
      "source": [
        "def run_gru(trial, seed=None):\n",
        "  print(f\"-----TRIAL {trial}:-----\")\n",
        "  # Use new seed\n",
        "  if seed is not None:\n",
        "      torch.manual_seed(seed)\n",
        "      np.random.seed(seed)\n",
        "\n",
        "  # Brand‑new model & optimiser\n",
        "  gru_model, gru_optimizer = init_gru()\n",
        "\n",
        "  # Train it\n",
        "  gru_model_losses = train_gru(gru_model, gru_optimizer)\n",
        "\n",
        "  # Optional: plot training loss\n",
        "  # plot_gru_loss(gru_model_losses)\n",
        "\n",
        "  # Evaluate and collect metrics\n",
        "  metrics = test_gru(gru_model)\n",
        "  print(\"------------------\\n\")\n",
        "  return metrics\n",
        "\n",
        "# Repeat\n",
        "gru_metrics = [run_gru(i + 1, seed=i) for i in range(config.num_trials)]\n",
        "gru_aucs, gru_praucs, gru_accs, \\\n",
        "gru_precisions, gru_recalls, gru_f1s = map(list, zip(*gru_metrics))\n",
        "\n",
        "print(\n",
        "    \"\\n[GRU RESULTS OVER TRIALS] \"\n",
        "    f\"AUC = {round(mean(gru_aucs), 4)} +/- {round(stdev(gru_aucs), 4)}, \"\n",
        "    f\"PRAUC = {round(mean(gru_praucs), 4)} +/- {round(stdev(gru_praucs), 4)}, \"\n",
        "    f\"ACC = {round(mean(gru_accs), 4)} +/- {round(stdev(gru_accs), 4)}, \"\n",
        "    f\"PRECISION = {round(mean(gru_precisions), 4)} +/- {round(stdev(gru_precisions), 4)}, \"\n",
        "    f\"RECALL = {round(mean(gru_recalls), 4)} +/- {round(stdev(gru_recalls), 4)}, \"\n",
        "    f\"F1 = {round(mean(gru_f1s), 4)} +/- {round(stdev(gru_f1s), 4)}\"\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
